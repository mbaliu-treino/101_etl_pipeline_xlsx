{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ETL with Python - Manipulating Large Masses of Data"
      ],
      "metadata": {
        "id": "qywvZXStwbTx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "_Este arquivo é o resultado do projeto guiado instruído pela DIO realizado em junho de 2024._"
      ],
      "metadata": {
        "id": "Er7TKwy3LxSX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **PROBLEMA**\n",
        "\n",
        "Uma organização recebe um conjunto de dados de um relatório de forma constante e é esperado que eles sejam organizados em um painel que sintetize todos eles de uma maneira clara. Esses dados de assinaturas são gerados diariamente e representam os produtos gerados por cada sistema distribuído em cada país. Como consequência, eles não possuem a mesma padronização.\n",
        "\n",
        "Ao final será entregue uma aplicação que faz a carga de arquivos XLSX em Bancos de Dados.\n",
        "\n",
        "Este estudo mostra **maturidade do processo** e **a visibilidade que se tem sobre os dados**.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PFT8ihsGHOtD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CONTEÚDO APRENDIDO\n",
        "\n",
        "* **Organização de um projeto de Tratamentos de Dados**\n",
        "    * Organização de as pastas\n",
        "    * Criação de ambiente de execução (venv)\n",
        "\n",
        "* Etapas de dados\n",
        "* Conectar nos Bancos de Dados\n",
        "* Garantir a confiabilidade dos dados\n",
        "* Garantir a rastreabilidade dos dados\n",
        "* Desenhar esquemas (tldraw - extensão no VSCode)"
      ],
      "metadata": {
        "id": "vnmk0CtqMUDL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CONCEITOS PARA ENTREVISTA**\n",
        "\n",
        "Para demonstrar MATURIDADE e CONHECIMENTO DO SENTIDO das pequenas partes.\n",
        "\n",
        "Muitas vezes, em entrevistas técnicas é comum a pesso achegar com ocódigo pronto. Muitas vezes é interessante explicar os principais conceitos (porque usar uma determinada solução? Como ela funciona? Para uso de VENV, organização do projeto e camadas de dados). **Demonstrar que sabe fazer o simples bem feito**.\n",
        "\n",
        "Então saber falar sobre o desenvolvimento de SOLUÇÕES e não somente de CÓDIGOS."
      ],
      "metadata": {
        "id": "2Yu3x_CmFdiq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Regras do Tratamento de dados**\n",
        "\n",
        "* Prezar pela confiabilidade e rastreabilidade dos dados"
      ],
      "metadata": {
        "id": "5TYaiNLdJ9EN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Camadas de Armazenamento**\n",
        "\n",
        "* RAW e READY\n",
        "* BRONZE, SILVER e GOLD"
      ],
      "metadata": {
        "id": "mnw2D--qxBCj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ambiente Virtual Execução/Desenvolvimento**"
      ],
      "metadata": {
        "id": "DF-Bg-2AxQVz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ETL de Dados**\n",
        "\n",
        "* cookiecutter - módulo de python\n",
        "\n"
      ],
      "metadata": {
        "id": "nfgBk0scxEic"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**UTM tag**\n",
        "\n",
        "UTM (Urchin Tracking Module ou Módulo de Rastreamento Urchin) é uma extensão de link de rastreamento que serve para entender a origem do tráfego.\n",
        "\n",
        "Através da UTM, é possível saber onde foi publicado e qual o conteúdo e campanha veiculados a determinado link"
      ],
      "metadata": {
        "id": "umdFmLw6Uzzm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Estrutura de Projeto de Tratamento de Dados\n",
        "\n",
        "* `src`\n",
        "    * _Tudo dentro do source é o código-fonte ou arquivos de textos, imagens etc. Tudo que estiver fora da `src` ou é documentação ou é configuração_\n",
        "    * `data`\n",
        "        * `raw`: dados brutos sem qualquer alteração\n",
        "        * `ready`: dados que já passaram por alguns script de tratamento do projeto. Então é onde os dados refinados são armazenados.\n",
        "    * `main.py`: se for só um arquivo ou uma aplicação inicial que chama todas as outras é possível usar um único arquivo.\n",
        "    * [`scripts`]: pasta opcional para quando hpa mais do que um único arquivo de execução da aplicação. Isso serve para separar a camada de dados da camada de códigos.\n",
        "\n",
        "\n",
        "Uma estrutura de projeto de API não é igual a uma estrutura de projeto de dados\n",
        "\n",
        "```\n",
        "data/\n",
        "|- raw/\n",
        "|- ready/\n",
        "|- main.log\n",
        "scripts/\n",
        "|- notebooks/\n",
        "|- main.py\n",
        "docs/\n",
        "|- how-to.md\n",
        "|- changelog.md\n",
        "```\n",
        "\n",
        "\"Transformação de data raw em dados refinados de negócios\""
      ],
      "metadata": {
        "id": "rah-9LqhdVSP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Padrão Arquitetural - MVC\n",
        "\n",
        "https://www.youtube.com/watch?v=9Ieh0yoiiqI\n",
        "\n",
        "Um padrão arquitetural é uma forma de estruturar uma aplicação, como um todo, com todas as suas funcionalidades. Um padrão é um desenho de camadas, principalmente, camadas lógicas.\n",
        "\n",
        "Um padrão arquitetural é importante é importante para planejar um Framework. Desta forma, ela ajuda a separar as partes da solução em camadas lógicas (pastas), contribuindo para a organização e independência entre estes grandes elementos da arquitetura. Isso segue o princípio do **desacoplamento**. Uma vantagem é a simplificação do processo de testes, em que pode-se testar cada um desses elementos individualmente. Então melhora a **TESTABILIDADE**.\n",
        "\n",
        "* CONTROLLER é a camada principal, também chamada de camada primária. Então ela é a porta de entrada para qualquer cliente. Por exemplo em um site, o cliente acessa através de uma rota o `controller` que irá carregar uma `view`. Esta camada possuirá códigos de orquestração de chamada de `models` quanto de `views`.\n",
        "\n",
        "* **VIEW**: esta camada é responsável por trabalhar todas as regras de visualização, ou seja, aspectos relacionados com a experiência com o usuário (regras de input e output; interface com o usuário).\n",
        "\n",
        "* **MODEL**: esta camada possui todas as lógicas da regra de negócio.\n",
        "\n",
        "\n",
        "Para fazer este direcionamento, a `controller`possuirá uma lógica/inteligência que irá carregar informações de um `model`, que será processadas para construir uma `view`. A resposta da `view` é enviada para o `Controller`, que enviará para o usuário."
      ],
      "metadata": {
        "id": "Ff9s9vsRdHw4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Git Flow\n",
        "\n",
        "1. Nova Feature\n",
        "    1. AMBIETE DE DESENVOLVIMENTO: Criar nova Branch LOCAL `<ftr|bug|done>/<..>`\n",
        "    2. Desenvolver a solução\n",
        "    3. UPDATE: Fazer o pull para o `main`\n",
        "    3. MERGE: Merge into main LOCAL\n",
        "    4. PULL REQUEST: Fazer o push para uma nova Branch, a partir do `main local` a uma nova Branch Remota. Depois fazer o Pull Request desta Branch"
      ],
      "metadata": {
        "id": "dXmAHda_RgCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TAREFAS\n",
        "\n",
        "* Consolidar diversos arquivos de planilhas em uma única fonte\n",
        "    * Extrair informações de strings\n",
        "    * Rastreabildiade dos dados - indicar qual a origem dos dados"
      ],
      "metadata": {
        "id": "XdPs31iATkSt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ROTEIRO DE PROJETO\n",
        "\n",
        "1. <font color=orange><b>Iniciar um Novo Projeto</b></font>\n",
        "    1. Abrir pasta\n",
        "        * `C:/www/python`\n",
        "    2. Nova pasta para o Projeto\n",
        "        * `mkdir make-data-netflix`\n",
        "    3. Abrir o VSCode na pasta\n",
        "        * `code make-data-netflix`\n",
        "2. <font color=orange><b>Documentação do Projeto</b></font>\n",
        "    1. Crição de Pasta\n",
        "        * `mkdir docs`\n",
        "        * `cd docs`\n",
        "        * `cat how-to.md`\n",
        "3. <font color=orange><b>Ambiente de Desenvolvimento</b></font>\n",
        "    1. Criação do ambiente\n",
        "        * `python -m venv .venv`\n",
        "    2. Ativação do ambiente\n",
        "        * `.venv/Scripts/activate`\n",
        "    3. Instalação de dependências\n",
        "        * `pip install pandas, openpyxl, xlsxwriter, requests, polars`\n",
        "        * `xlswriter` é mais performático na hora de escrever e permite operar de maneira mais atômica.\n",
        "        * `pip install -r requirements.txt`\n",
        "        * `poetry`\n",
        "    4. Git\n",
        "        * `.gitignore`\n",
        "            * adicionar a pasta `src/data/`\n",
        "            * adicionar a pasta `.venv/`\n",
        "3. <font color=orange><b>Preparação</b></font>\n",
        "    1."
      ],
      "metadata": {
        "id": "M3PPpIpdYrae"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Adicionar no Github\n",
        "2. Adicionar Github do projeto no <u>Repositório de Aulas</u>\n",
        "3. Criar"
      ],
      "metadata": {
        "id": "KA40LvpbMlVz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* [Repositório de dados](https://www.youtube.com/redirect?event=live_chat&redir_token=QUFFLUhqbjFNdkpmNGhMTzROX2lJWGhXMUVYOW0xZWFtUXxBQ3Jtc0ttbUphNDV1dklmUEE4VjlnTWpONTVGSV8zeHZ5UG9CVWlHRWlfM1lvc3B5OV9FXzVkT2lyVk9LYWZqX3d3OVJVRWhZUDNNVFEtVWpObWxpWXlockhNbDN5WmZWUFBSLXdYRkJIaUduZFJCTE9VUTZ0RQ&q=https%3A%2F%2Fgithub.com%2Fdigitalinnovationone%2Fnetflix-dataset%2Ftree%2Fmain%2Fraw)"
      ],
      "metadata": {
        "id": "n6wk8PJGeT4J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GISZcLz7vvJF"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import glob\n",
        "\n",
        "# Caminho para ler o caminho\n",
        "folder_path = os.path.join(os.getcwd(), 'src', 'data', 'raw')\n",
        "\n",
        "# Lista de arquivos\n",
        "glob_unix_pattern = os.path.join(folder_path, '*.xlsx')\n",
        "excel_files = glob.glob( glob_unix_pattern )\n",
        "\n",
        "if not excel_files:\n",
        "    print('No file found in the specified folder')\n",
        "    raise ValueError('No file found in the specified folder')\n",
        "else:\n",
        "    # Dataframe - data in memory\n",
        "    dfs = []\n",
        "\n",
        "    for excel_file in excel_files:\n",
        "        try:\n",
        "            # Leitura de XLSX\n",
        "            df_temp = pd.read_excel(excel_file)\n",
        "\n",
        "            # Principio da rastreabilidade\n",
        "            # Nome do arquivo\n",
        "            df_temp['filename'] = os.path.basename(excel_file)\n",
        "\n",
        "            # Extração da localização da tabela\n",
        "            df_temp['location'] = df_temp['filename'].str.split('_')[-1]\n",
        "            df_temp['location'] = df_temp['location'].str.split('.')[0].str.lower()\n",
        "\n",
        "            # Extração de info sobre campaign - UTM\n",
        "            # df_temp = df_temp['utm_link'].str.split('utm_source=').str[-1]\n",
        "            df_temp = df_temp['utm_link'].str.extract(r'utm_campaign=(.*)')  # no extract recebe um padrão de regex\n",
        "\n",
        "            df_temp['campaign'] = df_temp[df_temp['utm_source'].notnull()]['utm_campaign']\n",
        "            df_temp['utm_source'] = df_temp[df_temp['utm_source'].notnull()]['utm_source']\n",
        "\n",
        "            # Conjunto de tabelas\n",
        "            dfs.append(df_temp)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f'Erro ao ler o arquivo {excel_file}: {e}')\n",
        "\n",
        "\n",
        "if len(dfs) != 0:\n",
        "    # Concatena todas as tabelas contidas no dfs em um única tabela\n",
        "    result = pd.concat( dfs, ignore_index=True)\n",
        "\n",
        "    # Cria o caminho da nova tabela tratada\n",
        "    output_file = os.path.join(os.getcwd(), 'src', 'data', 'ready', 'netflix_data_cleaned.xlsx')\n",
        "\n",
        "    # Criação da instância com especificação de motor (opcional)\n",
        "    writer = pd.ExcelWriter(output_file, engine='xlsxwriter')  # mudança do motor de escrita. XLSXWRITER pode ser mais performático\n",
        "\n",
        "    # Persistência da tabela em um arquivo excel e fechamento do arquivo\n",
        "    result.to_excel(writer, sheet_name='netflix_data_cleaned', index=False)\n",
        "    writer.save()\n",
        "    writer.close()\n",
        "else:\n",
        "    print('Nenhum dado para ser salvo')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BOAS PRÁTICAS\n",
        "\n",
        "```python\n",
        "```\n",
        "\n",
        "* Código robusto entre sistemas\n",
        "\n",
        "```python\n",
        "# Caminho para ler o caminho\n",
        "folder_path = os.path.join(os.getcwd(), 'src', 'data', 'raw')\n",
        "```\n",
        "\n",
        "* Listagem de arquivos com filtro\n",
        "\n",
        "```python\n",
        "# Lista de arquivos\n",
        "glob_unix_pattern = os.path.join(folder_path, '*.xlsx')\n",
        "excel_files = glob.glob( glob_unix_pattern )\n",
        "```\n",
        "\n",
        "* Design por Contrato\n",
        "\n",
        "```python\n",
        "if not excel_files:\n",
        "    print('No file found in the specified folder')\n",
        "    raise ValueError('No file found in the specified folder')\n",
        "```\n",
        "\n",
        "* Tolerante a falhas\n",
        "\n",
        "```python\n",
        "except Exception as e:\n",
        "    print(f'Erro ao ler o arquivo {excel_file}: {e}')\n",
        "```\n",
        "\n",
        "* Operações Pandas\n",
        "\n",
        "```python\n",
        "# Extração da localização da tabela\n",
        "df_temp['location'] = df_temp['filename'].str.split('_')[-1]\n",
        "df_temp['location'] = df_temp['location'].str.split('.')[0].str.lower()\n",
        "\n",
        "# Extração de info sobre campaign - UTM\n",
        "# df_temp = df_temp['utm_link'].str.split('utm_source=').str[-1]\n",
        "df_temp = df_temp['utm_link'].str.extract(r'utm_campaign=(.*)')  # no extract recebe um padrão de regex\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "```python\n",
        "```"
      ],
      "metadata": {
        "id": "Fcf-Fj_S8J7g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# README\n",
        "\n"
      ],
      "metadata": {
        "id": "RWNNJjzaRUlk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Definição dos Requisitos de Entrada e Saída\n",
        "2. Definição dos Padrões e das Transformações necessárias\n",
        "    * Definição da Organização do Projeto\n",
        "    * Definição das Camadas de Especilização dos Dados\n",
        "    * Virtualização do ambiente de desenvolvimento\n",
        "1. Definição das sprints\n",
        "    * Processo incremental de melhorias\n",
        "3. Construção do primeiro protótipo\n",
        "\n",
        "<font color=orange>SPRINTS TEMÁTICAS</font>\n",
        "\n",
        "* Modulação da solução\n",
        "* Controle do fluxo, resiliência e contratos da solução\n",
        "    * Gerenciamento de erros\n",
        "* Qualidade de Dados\n",
        "* Rastreabilidade dos dados\n",
        "    * Nomes dos arquivos\n",
        "    * Link UTM\n",
        "* Performance das soluções e alternativas\n",
        "* Encapsulamento da solução\n",
        "* Conteinerização\n",
        "* Orquestração\n",
        "* Sistema de Logs"
      ],
      "metadata": {
        "id": "YrceLbdkGTgn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ROTEIRO DE DESENVOLVIMENTO DE SOLUÇÃO DE ENGENHARIA DE DADOS**\n",
        "\n",
        "1. PLANEJAMENTO DO PROBLEMA\n",
        "\n",
        "    * A. **Entendimento do Problema**: _Definição clara do problema que está tentando resolver com a solução de dados._\n",
        "    * B. **Entendimento dos Requisitos de Negócio (Objetivos)**: _Definição dos objetivos da solução de dados._\n",
        "        * Quais problemas de negócio precisa ser resolvido?\n",
        "    * C. **Definição dos Requisitos de Dados**: _Definição de parâmetros esperados de desempenho_.\n",
        "        * Qual o volume, a variedade e a velocidade dos dados que serão processados?\n",
        "        * Quais as fontes de dados?\n",
        "        * Qual a frequencia de atualização?\n",
        "    * D. **Arquitetura da Solução**: Escolha das tecnologias e ferramentas adequadas para cada etapa do pipeline de dados, considerando aspectos como escalabilidade, performance, custo e segurança.\n",
        "        * **Armazenamento de Dados**: Data Lakes (S3, Azure Glob Storage, MinIO); Data Warehouse (Redshift, BigQuery), Bancos de Dados NoSQL (Cassandra, MongoDB, ScyllaDB)\n",
        "        * **Processamento de Dados**: Ferramentas de ETL (Apache Spark, Apache Beam), plataformas de streaming / mensageria (Kafka, Kinesis)\n",
        "        * **Orquestração de Pipelines**: Ferramentas como Apache Airflow ou Luigi.\n",
        "    \n",
        "2. COLETA E INGESTÃO DE DADOS\n",
        "\n",
        "    * Construa Pipelines de Ingestão: Crie pipelines robustos e escaláveis para coletar dados de diversas fontes (bancos de dados, APIs, arquivos, sensores, etc.).\n",
        "    * Garanta a Qualidade dos Dados: Implemente mecanismos de validação e limpeza de dados durante a ingestão para garantir a consistência e a confiabilidade dos dados.\n",
        "\n",
        "3. ARMAZENAMENTO E PROCESSAMENTO\n",
        "\n",
        "*Modelagem de Dados: Defina a estrutura dos dados para armazenamento e processamento, considerando os requisitos de análise e as ferramentas a serem utilizadas.\n",
        "Processamento em Batch ou Streaming: Escolha a abordagem de processamento mais adequada (batch ou streaming) com base na frequência de atualização dos dados e nos requisitos de tempo real.\n",
        "Otimização de Performance: Utilize técnicas de otimização para garantir o processamento eficiente de grandes volumes de dados.\n",
        "\n"
      ],
      "metadata": {
        "id": "AsXHpj0EDH-V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Um roteiro para uma solução de engenharia de dados pode ser mais específico, focando em aspectos como infraestrutura, processamento de dados em larga escala e pipelines de dados. Aqui está um roteiro detalhado:\n",
        "\n",
        "1. Definição de Requisitos e Arquitetura:\n",
        "\n",
        "Entenda os Requisitos de Negócio: Defina os objetivos da solução de dados. Quais problemas de negócio ela precisa resolver? Quais insights ela precisa fornecer?\n",
        "Defina os Requisitos de Dados: Determine o volume, a variedade e a velocidade dos dados que serão processados. Quais as fontes de dados? Qual a frequência de atualização?\n",
        "Desenhe a Arquitetura da Solução: Escolha as tecnologias e ferramentas adequadas para cada etapa do pipeline de dados, considerando aspectos como escalabilidade, performance, custo e segurança. Isso pode incluir:\n",
        "Armazenamento de Dados: Data lakes (S3, Azure Data Lake), data warehouses (Redshift, BigQuery), bancos de dados NoSQL (Cassandra, MongoDB).\n",
        "Processamento de Dados: Ferramentas de ETL (Apache Spark, Apache Beam), plataformas de streaming (Kafka, Kinesis).\n",
        "Orquestração de Pipelines: Ferramentas como Apache Airflow ou Luigi.\n",
        "2. Coleta e Ingestão de Dados:\n",
        "\n",
        "Construa Pipelines de Ingestão: Crie pipelines robustos e escaláveis para coletar dados de diversas fontes (bancos de dados, APIs, arquivos, sensores, etc.).\n",
        "Garanta a Qualidade dos Dados: Implemente mecanismos de validação e limpeza de dados durante a ingestão para garantir a consistência e a confiabilidade dos dados.\n",
        "3. Armazenamento e Processamento de Dados:\n",
        "\n",
        "Modelagem de Dados: Defina a estrutura dos dados para armazenamento e processamento, considerando os requisitos de análise e as ferramentas a serem utilizadas.\n",
        "Processamento em Batch ou Streaming: Escolha a abordagem de processamento mais adequada (batch ou streaming) com base na frequência de atualização dos dados e nos requisitos de tempo real.\n",
        "Otimização de Performance: Utilize técnicas de otimização para garantir o processamento eficiente de grandes volumes de dados.\n",
        "4. Governança e Segurança de Dados:\n",
        "\n",
        "Implemente Políticas de Governança: Defina políticas de acesso, controle de qualidade, privacidade e segurança dos dados.\n",
        "Garanta a Segurança dos Dados: Utilize mecanismos de autenticação, autorização e criptografia para proteger os dados em todas as etapas do pipeline.\n",
        "5. Monitoramento e Operação da Solução:\n",
        "\n",
        "Implemente Monitoramento Contínuo: Monitore a saúde e a performance do pipeline de dados, coletando métricas e logs para identificar e solucionar problemas rapidamente.\n",
        "Automatize Tarefas Operacionais: Utilize ferramentas de automação para simplificar tarefas como provisionamento de recursos, implantação de código e gerenciamento de configuração.\n",
        "6. Documentação e Comunicação:\n",
        "\n",
        "Documente a Solução: Crie documentação detalhada sobre a arquitetura, os pipelines de dados, as ferramentas utilizadas e os processos operacionais.\n",
        "Comunique os Resultados: Mantenha as partes interessadas informadas sobre o progresso do projeto, os desafios encontrados e os resultados alcançados.\n",
        "Este roteiro fornece uma visão geral das etapas envolvidas na construção de uma solução de engenharia de dados. A implementação específica de cada etapa dependerá dos requisitos do projeto, das tecnologias escolhidas e das habilidades da equipe."
      ],
      "metadata": {
        "id": "XiYJHSnyDW_9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ROTEIRO DE DESENVOLVIMENTO DE SOLUÇÃO DE DADOS**\n",
        "\n",
        "1. PLANEJAMENTO DO PROBLEMA\n",
        "\n",
        "    * A. **Entendimento do Problema**: _Definição clara do problema que está tentando resolver com a solução de dados.\n",
        "        * Quais perguntas busca responder?\n",
        "        * Quais insights deseja obter?\n",
        "    * B. **Definição dos Objetivos**: _Estabelecimento de objetivos mensuráveis e específicos para o projeto_.\n",
        "        * O que espera alcançar com o projeto?"
      ],
      "metadata": {
        "id": "JqlFP4dj8Rfr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui está um roteiro de checklist para as etapas de desenvolvimento de uma solução de dados, baseado no script que você forneceu:\n",
        "\n",
        "1. Planejamento e Definição do Problema:\n",
        "\n",
        "Entendimento do Problema: Defina claramente o problema que você está tentando resolver com a solução de dados. Quais perguntas você busca responder? Quais insights você deseja obter?\n",
        "Definição dos Objetivos: Estabeleça objetivos mensuráveis e específicos para o projeto. O que você espera alcançar com a análise dos dados?\n",
        "Identificação das Fontes de Dados: Determine quais fontes de dados serão utilizadas e como os dados serão coletados.\n",
        "Definição da Arquitetura da Solução: Planeje a estrutura da solução, incluindo as ferramentas e tecnologias a serem utilizadas (bancos de dados, plataformas de processamento, ferramentas de visualização, etc.).\n",
        "2. Coleta e Armazenamento de Dados:\n",
        "\n",
        "Extração dos Dados: Utilize bibliotecas como Pandas para extrair os dados das fontes identificadas (arquivos XLSX, APIs, bancos de dados, etc.).\n",
        "Limpeza e Pré-processamento dos Dados: Realize a limpeza dos dados, tratando valores ausentes, inconsistências e erros.\n",
        "Armazenamento dos Dados: Armazene os dados em um formato adequado para análise, como um banco de dados relacional (MySQL, PostgreSQL) ou um data lake (Hadoop, S3).\n",
        "3. Processamento e Transformação de Dados:\n",
        "\n",
        "Transformação dos Dados: Utilize ferramentas de ETL (Extract, Transform, Load) para transformar os dados em um formato adequado para análise. Isso pode incluir agregação, filtragem, junção de tabelas, etc.\n",
        "Criação de Features: Engenharia de recursos para criar novas variáveis a partir dos dados existentes, que podem melhorar a performance dos modelos de machine learning.\n",
        "Normalização e Padronização dos Dados: Aplique técnicas de normalização e padronização para garantir que os dados estejam em uma escala adequada para análise.\n",
        "4. Análise e Modelagem de Dados:\n",
        "\n",
        "Análise Exploratória de Dados: Realize uma análise exploratória dos dados para entender as distribuições, identificar padrões e gerar insights.\n",
        "Modelagem de Dados: Utilize técnicas de machine learning para construir modelos preditivos ou descritivos, dependendo dos objetivos do projeto.\n",
        "Validação dos Modelos: Avalie a performance dos modelos utilizando métricas apropriadas e técnicas de validação cruzada.\n",
        "5. Visualização e Comunicação dos Resultados:\n",
        "\n",
        "Criação de Visualizações: Utilize ferramentas de visualização (Matplotlib, Seaborn, Plotly) para criar gráficos e dashboards que comuniquem os insights de forma clara e eficaz.\n",
        "Interpretação dos Resultados: Interprete os resultados da análise e os insights gerados pelos modelos, traduzindo-os em linguagem de negócios.\n",
        "Comunicação dos Resultados: Apresente os resultados e insights de forma clara e concisa para a audiência, utilizando relatórios, apresentações ou dashboards interativos.\n",
        "6. Implantação e Monitoramento da Solução:\n",
        "\n",
        "Implantação da Solução: Implemente a solução em um ambiente de produção, garantindo que ela seja escalável e robusta.\n",
        "Monitoramento da Solução: Monitore a performance da solução e os resultados dos modelos, realizando ajustes e atualizações quando necessário.\n",
        "7. Documentação:\n",
        "\n",
        "Documente todo o processo: Desde a definição do problema até a implantação da solução, documentando as decisões tomadas, os códigos desenvolvidos e os resultados obtidos.\n",
        "Este roteiro serve como um guia geral e pode ser adaptado de acordo com as necessidades específicas do seu projeto. Lembre-se de que a comunicação e a colaboração entre as equipes são essenciais para o sucesso de qualquer projeto de dados."
      ],
      "metadata": {
        "id": "H1rD0vKP-JQZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ETL Pipeline XLSX\n",
        "\n",
        "Este projeto consiste em um processo de ETL simples para consolidar dados de múltiplos arquivos XLSX em um único arquivo.\n",
        "\n",
        "O código Python utiliza bibliotecas como Pandas, Openpyxl, Xlsxwriter para manipular os dados localizados em um repositório local\n",
        "\n",
        "## Funcionalidades\n",
        "\n",
        "* Extração de dados de vários arquivos XLSX em um pasta específica (`src/data/raw`)\n",
        "* Adição de informações de rastreabilidade, como nome do arquivo, localidade e informações de campanha a partir de links UTM\n",
        "* Concatenação dos dados em um único DataFrame\n",
        "* Persistência do DataFrame em um novo arquivo XLSX\n",
        "\n",
        "\n",
        "## Como usar\n",
        "\n",
        "* <font color=red>Conteinerizar solução</font>\n",
        "\n",
        "1. Preparação do ambiente de execução\n",
        "\n",
        "`pip install -r requirements.txt`\n",
        "\n",
        "2. Os arquivos XLSX devem estar na pasta `src/data/raw`.\n",
        "\n",
        "3. Execute o script python\n",
        "\n",
        "4. O arquivo consolidado será salvo em `src/data/ready/data_cleaned.xlsx`\n",
        "\n",
        "\n",
        "## Conclusões\n",
        "\n",
        "Este projeto serve como base para construir pipelines de dados mais complexos e pode ser adaptado para diferentes fontes de dados e requisitos de transformação."
      ],
      "metadata": {
        "id": "-dH3qdAURW34"
      }
    }
  ]
}